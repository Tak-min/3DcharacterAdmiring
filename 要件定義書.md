承知いたしました。
前回の提示内容、特にテンプレート形式や冗長な記述に関しまして、多大なご不快とご面倒をおかけしましたこと、深くお詫び申し上げます。ご指摘いただいた内容は真摯に受け止め、思考を全面的に改めました。

これより、ご命令いただいた通り、一切の曖昧さを排除し、プロジェクトの最終形までを一気通貫で見通せる完全な設計図として、以下のドキュメントを提出いたします。

3Dキャラクター対話Webアプリ：統合開発ドキュメント (v3.0) - マスタープラン
1. プロジェクトビジョン
本プロジェクトの目標は、単なる対話ボットではなく、ユーザーの発言に含まれる感情や意図を汲み取り、生きているかのようなアニメーションと音声で応答するAIコンパニオンを創造することである。参考指標はGrokのAIコンパニオン「Ani」とし、そのレベルの感情表現とインタラクティブ性をWeb上で実現する。

ユーザーは、自身の感情の機微を理解してくれるパートナーとして、キャラクターとの対話に没入できる体験を得る。

2. システムアーキテクチャ
本システムは、責務が明確に分離された3つの主要コンポーネントで構成される。

フロントエンド (Cloudflare Pages):

責務: UI/UX、3Dレンダリング (A-Frame/Three.js)、ユーザー操作の受付、Auth0を利用した認証リダイレクト、バックエンドAPIへのリクエスト送信。

技術: React (or Vue/Svelte), A-Frame, Three.js, Auth0 SPA SDK。

バックエンド (PaaS/サーバー on Docker):

責務: ビジネスロジックの実行、外部AI API群との連携、対話セッション管理、フロントエンドへの応答生成。

技術: FastAPI (Python), Docker, Uvicorn。

認証サービス (Auth0):

責務: ユーザーの新規登録、ログイン認証、アクセストークンの発行と検証。バックエンドAPIの保護。

3. インタラクション・シーケンス
ユーザーの入力からキャラクターの応答までのデータフローは、以下の通りである。

ユーザー操作: フロントエンドでユーザーがテキスト入力または音声入力を行う。

認証: APIリクエスト時、フロントエンドはAuth0から取得したアクセストークンをAuthorizationヘッダーに付与する。

APIリクエスト: フロントエンドは、入力内容とアクセストークンをバックエンドの /interact エンドポイントへ送信する。

トークン検証: バックエンドは、リクエストされたAPIエンドポイントの保護ガードとして機能し、Auth0の公開鍵を用いてアクセストークンの正当性を検証する。

リアルタイム文字起こし (音声の場合): AssemblyAI APIへ音声データをストリーミングし、テキストに変換する。

思考と感情分析: 変換されたテキストをGemini APIへ送信。応答メッセージと共に、文脈から判断される感情データ（例: { "emotion": "joy", "intensity": 0.8 }）を生成させる。

音声合成: 生成された応答メッセージをElevenLabs APIへ送信し、キャラクターボイスを生成する。

APIレスポンス: バックエンドは、音声データ、応答テキスト、そして感情データを一つのJSONオブジェクトとしてフロントエンドに返す。

キャラクターの応答: フロントエンドはAPIレスポンスを受け取る。

音声データを再生する。

応答テキストを画面に表示する。

感情データを基に、定義されたマッピングルールに従って最適なアニメーション（例: "joy" -> "Happy_Idle"）をトリガーする。

4. API設計
バックエンドは以下のAPIエンドポイントを提供する。全ての機密エンドポイントはAuth0による認証を必須とする。

Endpoint: /interact

Method: POST

Protection: Required (Auth0 Access Token)

Request Body:

JSON

{
  "inputType": "text" | "audio",
  "data": "string (text or base64 encoded audio data)",
  "sessionId": "string (optional)"
}
Success Response (200 OK):

JSON

{
  "responseText": "string",
  "audioContent": "string (base64 encoded audio data)",
  "emotionData": {
    "emotion": "joy" | "sadness" | "agreement" | "surprise" | "neutral" | "anger",
    "intensity": "float (0.0 to 1.0)"
  },
  "sessionId": "string"
}
Endpoint: /health

Method: GET

Protection: None

Success Response (200 OK):

JSON

{ "status": "ok" }
5. 開発コード憲章
原則: 明確さ、シンプルさ、自己責任を重んじる。コードは機能仕様の一部であり、最高水準の品質を維持する。

スタイル: バックエンドはBlack, isort, Ruffによる静的解析を必須とする。フロントエンドはPrettier, ESLintを適用する。

バージョン管理: Conventional Commits規約に従ったコミットメッセージを必須とし、developブランチへのマージは必ずプルリクエスト経由で行う。

6. マスタープロンプト
To: Code Agent
From: Project Architect
Subject: [Project Genesis] Build a full-stack, emotionally intelligent 3D AI companion web application.

# Mission Directive
Your primary objective is to construct a complete, full-stack web application as defined in the above documentation (v3.0). You will write all necessary code for the backend, frontend, and infrastructure configuration. The final product must be a deployable, high-quality application. This is not a partial task; you are responsible for the entire project genesis.

# Phase 1: Backend Scaffolding & Security Foundation (FastAPI & Auth0)
Your first task is to build the core backend structure, including robust security with Auth0.

Action Items:

Create the project structure:

/app
|-- /api
|   |-- /routes
|   |   |-- __init__.py
|   |   `-- interact.py
|   `-- __init__.py
|-- /core
|   |-- __init__.py
|   |-- config.py
|   `-- security.py
|-- /services
|   |-- __init__.py
|   |-- assemblyai_service.py
|   |-- gemini_service.py
|   `-- elevenlabs_service.py
|-- /models
|   |-- __init__.py
|   `-- schemas.py
|-- __init__.py
`-- main.py
.dockerignore
Dockerfile
pyproject.toml
README.md
pyproject.toml: Define all dependencies.

Ini, TOML

[tool.poetry]
name = "ai-companion-backend"
version = "0.1.0"
description = ""
authors = ["Your Name <you@example.com>"]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.111.0"
uvicorn = {extras = ["standard"], version = "^0.29.0"}
pydantic = "^2.7.1"
python-dotenv = "^1.0.1"
requests = "^2.31.0"
google-generativeai = "^0.5.4"
assemblyai = "^0.23.0"
websockets = "^12.0"
# For Auth0
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
fastapi-auth0 = "^0.5.0" # Assuming a community library or direct implementation
core/config.py: Implement settings management using Pydantic's BaseSettings to load environment variables (e.g., AUTH0_DOMAIN, AUTH0_API_AUDIENCE, GEMINI_API_KEY, etc.).

core/security.py: Implement Auth0 token validation. This is the most critical security component. Create a dependency that can be used to protect endpoints. It must fetch the JWKS from your Auth0 domain, decode the JWT, and verify its claims.

models/schemas.py: Define all Pydantic models for API requests and responses as specified in the "API設計" section.

main.py: Set up the main FastAPI application instance. Include CORS middleware to allow requests from the frontend's domain. Mount the API routers.

Dockerfile: Create a production-ready, multi-stage Dockerfile to build and run the application.

# Phase 2: Core Interaction Logic & AI Service Integration
Now, build the brain of the application.

Action Items:

Service Stubs: Create classes in each services/*.py file (GeminiService, AssemblyAIService, ElevenLabsService). Each class should have methods that encapsulate the logic for calling the respective external APIs. Use the requests library or their official Python clients.

Gemini Service & Emotion Logic: In gemini_service.py, implement a method that takes user text and returns not only a text response but also a structured emotion object (e.g., {"emotion": "...", "intensity": ...}). You must instruct the Gemini model via its system prompt to perform this analysis and return JSON as part of its response.

Implement the /interact Endpoint: In api/routes/interact.py, create the POST endpoint. It must be protected by the Auth0 security dependency from Phase 1.

This endpoint will orchestrate the calls to the AI services.

It should handle both "text" and "audio" inputType.

It will call the services in the correct sequence: AssemblyAI (if audio) -> Gemini -> ElevenLabs.

It must return the final, structured JSON response.

# Phase 3: Emotion-to-Animation Mapping Logic
This phase gives the character its soul.

Action Items:

Create a new module core/animation_mapper.py.

Inside this module, define a function get_animation_for_emotion(emotion_data: dict) -> str.

Implement the mapping logic: This function will contain a dictionary or a more complex rule-based system. It must map the emotion string and intensity float from Gemini's response to a specific animation clip name (string).

You are to devise this mapping. Consider a rich set of emotions (joy, sadness, agreement, surprise, neutral, anger, curiosity, thoughtful).

Map these to a plausible set of animation names (e.g., Idle_Happy, Nod_Head_Yes, Shake_Head_No, Look_Around_Surprised, Thinking_Pose, Idle_Sad).

The intensity can be used to select variants (e.g., intensity > 0.7 for Laugh vs. Smile).

Integrate into the /interact endpoint: Modify the endpoint to call this mapper function. Add the resulting animation name to the JSON response sent to the frontend.

Updated API Response:

JSON

{
  "responseText": "string",
  "audioContent": "string (base64 encoded audio data)",
  "emotionData": { ... },
  "animationName": "string" // <-- ADD THIS
}
# Phase 4: Frontend Implementation (React & A-Frame/Three.js)
Build the user-facing experience.

Action Items:

Set up a React project: Use create-react-app or Vite.

Auth0 Integration: Implement the @auth0/auth0-react SDK. Create login/logout buttons and a mechanism to get the access token for API calls. The application should be protected, requiring login to access the main interaction screen.

3D Scene Component: Create a React component that uses A-Frame or react-three-fiber to render a 3D scene and load a placeholder 3D model (e.g., a simple humanoid .glb file).

State Management: Use a state management solution (e.g., Zustand, Redux Toolkit) to handle application state, including conversation history, loading status, and authentication status.

Interaction UI:

Create a component for text/voice input. For voice, use the Web Speech API (MediaRecorder) to capture microphone audio.

Implement the API call logic to the backend /interact endpoint, attaching the Auth0 token.

Animation Player: Upon receiving the API response, your 3D component must be able to find and play the animation specified by the animationName field. You will need to write logic to manage the animation mixer in Three.js.

# Phase 5: Finalization & Deployment Configuration
Prepare the project for deployment.

Action Items:

CORS Configuration: Ensure the FastAPI CORS middleware is correctly configured to only accept requests from your planned Cloudflare Pages domain.

Environment Variables: Create .env.example files for both the frontend and backend, listing all required environment variables.

Deployment Scripts: Add scripts to package.json for building the React app.

README.md: Write a comprehensive README.md file detailing the entire project setup, environment variable configuration, and deployment steps for both frontend (Cloudflare Pages) and backend (Docker on a PaaS).

This directive is absolute. Proceed with implementation. No further clarification should be required.